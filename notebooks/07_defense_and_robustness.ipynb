{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNAkdGuhztDvAuFbBz+Wyh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#Imports & Device\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import segmentation_models_pytorch as smp\n","\n","import numpy as np\n","import glob\n","from PIL import Image\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","\n","import cv2\n","from io import BytesIO\n","import matplotlib.pyplot as plt\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", DEVICE)"],"metadata":{"id":"ta26BMLsRnGx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Dataset (Train + Val)\n","IMG_SIZE = 256\n","NUM_CLASSES = 5\n","\n","class MapDataset(Dataset):\n","    def __init__(self, img_dir, lbl_dir):\n","        self.img_paths = sorted(glob.glob(img_dir + \"/*.jpg\"))\n","        self.lbl_paths = sorted(glob.glob(lbl_dir + \"/*.txt\"))\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        img = Image.open(self.img_paths[idx]).convert(\"RGB\")\n","        img = img.resize((IMG_SIZE, IMG_SIZE))\n","        arr = np.array(img) / 255.0\n","        arr = arr.transpose(2,0,1).astype(\"float32\")\n","        label = int(open(self.lbl_paths[idx]).read().strip())\n","        return torch.tensor(arr), torch.tensor(label)"],"metadata":{"id":"GZPgy-P3RsQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TRAIN_DIR = \"data/train\"\n","VAL_DIR   = \"data/val_1\"\n","\n","train_ds = MapDataset(f\"{TRAIN_DIR}/images\", f\"{TRAIN_DIR}/labels\")\n","val_ds   = MapDataset(f\"{VAL_DIR}/images\",   f\"{VAL_DIR}/labels\")\n","\n","train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n","val_dl   = DataLoader(val_ds, batch_size=1, shuffle=False)\n","\n","print(\"Train samples:\", len(train_ds))\n","print(\"Val samples:\", len(val_ds))"],"metadata":{"id":"GKKhBjp9RslA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load base model\n","base_model = smp.Unet(\n","    encoder_name=\"resnet34\",\n","    encoder_weights=None,\n","    in_channels=3,\n","    classes=NUM_CLASSES\n",").to(DEVICE)\n","\n","base_model.load_state_dict(\n","    torch.load(\"models/unet_resnet34.pth\", map_location=DEVICE)\n",")\n","base_model.eval()\n","\n","print(\"âœ… Base model loaded\")"],"metadata":{"id":"jPU1L_muR1Ql"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#FGSM Generator\n","def fgsm_generate(model, x, y, eps):\n","    x_adv = x.clone().detach().to(DEVICE)\n","    x_adv.requires_grad = True\n","\n","    out = model(x_adv).mean(dim=(2,3))\n","    loss = F.cross_entropy(out, y)\n","\n","    model.zero_grad()\n","    loss.backward()\n","\n","    adv = x_adv + eps * x_adv.grad.sign()\n","    return torch.clamp(adv, 0, 1).detach()"],"metadata":{"id":"6_GfoETSR_0w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#FGSM Adversarial Fine-Tuning Loop\n","adv_model = smp.Unet(\n","    encoder_name=\"resnet34\",\n","    encoder_weights=None,\n","    in_channels=3,\n","    classes=NUM_CLASSES\n",").to(DEVICE)\n","\n","adv_model.load_state_dict(\n","    torch.load(\"models/unet_resnet34.pth\", map_location=DEVICE)\n",")\n","\n","optimizer = torch.optim.Adam(adv_model.parameters(), lr=5e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","EPOCHS_ADV = 3\n","EPS_TRAIN = 0.02\n","\n","print(\"ðŸ›¡ï¸ Starting FGSM adversarial fine-tuning...\\n\")\n","\n","for epoch in range(EPOCHS_ADV):\n","    adv_model.train()\n","    running_loss = 0.0\n","\n","    for imgs, labels in tqdm(train_dl, desc=f\"Adv Epoch {epoch+1}/{EPOCHS_ADV}\"):\n","        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n","\n","        # Generate FGSM adversarial examples\n","        adv_imgs = fgsm_generate(adv_model, imgs, labels, EPS_TRAIN)\n","\n","        optimizer.zero_grad()\n","\n","        out_clean = adv_model(imgs).mean(dim=(2,3))\n","        out_adv   = adv_model(adv_imgs).mean(dim=(2,3))\n","\n","        loss = 0.5 * criterion(out_clean, labels) + \\\n","               0.5 * criterion(out_adv, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1} | Avg Loss: {running_loss/len(train_dl):.4f}\")"],"metadata":{"id":"eowuLEueSF6K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save FGSM-Trained Model\n","torch.save(\n","    adv_model.state_dict(),\n","    \"models/unet_resnet34_adv.pth\"\n",")\n","\n","print(\"âœ… FGSM adversarially trained model saved\")"],"metadata":{"id":"AslDmLOPSKe7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#FGSM Evaluation Function\n","def eval_fgsm(model, dataloader, eps):\n","    correct, total = 0, 0\n","\n","    for x, y in dataloader:\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","        adv = fgsm_generate(model, x, y, eps)\n","\n","        with torch.no_grad():\n","            out = model(adv).mean(dim=(2,3))\n","            pred = out.argmax(dim=1)\n","\n","        correct += (pred == y).sum().item()\n","        total += 1\n","\n","    return 100 * correct / total"],"metadata":{"id":"QG_DMLxSR8SP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#FGSM Results (Base vs Adv-Trained)\n","EPS_LIST = [0.0, 0.02, 0.04, 0.06, 0.08]\n","\n","base_results = []\n","adv_results  = []\n","\n","for eps in EPS_LIST:\n","    base_results.append(eval_fgsm(base_model, val_dl, eps))\n","    adv_results.append(eval_fgsm(adv_model,  val_dl, eps))\n","print(\"\\nFGSM Robustness Comparison\")\n","print(\"Epsilon | Base (%) | FGSM-Trained (%)\")\n","print(\"-------------------------------------\")\n","\n","for i, eps in enumerate(EPS_LIST):\n","    print(f\"{eps:6.2f} | {base_results[i]:8.2f} | {adv_results[i]:14.2f}\")"],"metadata":{"id":"p1gn5CsUSUgL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#PGD Attack Function\n","def pgd_attack(model, x, y, eps=0.04, alpha=0.005, steps=20):\n","    model.eval()\n","\n","    x_orig = x.clone().detach().to(DEVICE)\n","    x_adv = x_orig.clone()\n","\n","    for _ in range(steps):\n","        x_adv.requires_grad = True\n","\n","        out = model(x_adv).mean(dim=(2,3))\n","        loss = F.cross_entropy(out, y)\n","\n","        model.zero_grad()\n","        loss.backward()\n","\n","        # PGD update\n","        x_adv = x_adv + alpha * x_adv.grad.sign()\n","\n","        # Project back to epsilon-ball\n","        perturb = torch.clamp(x_adv - x_orig, -eps, eps)\n","        x_adv = torch.clamp(x_orig + perturb, 0, 1).detach()\n","\n","    return x_adv"],"metadata":{"id":"tqCP6PTeQCrD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Hybrid Defense\n","def jpeg_def(arr, quality=40):\n","    im = Image.fromarray(arr)\n","    buf = BytesIO()\n","    im.save(buf, format=\"JPEG\", quality=quality)\n","    buf.seek(0)\n","    return np.array(Image.open(buf).convert(\"RGB\"))\n","\n","def gaussian_def(arr, k=3):\n","    return cv2.GaussianBlur(arr, (k,k), 0)\n","\n","def median_def(arr, k=3):\n","    return cv2.medianBlur(arr, k)\n","\n","def bit_def(arr, bits=5):\n","    shift = 8 - bits\n","    return ((arr >> shift) << shift)\n","\n","def hybrid_defense(arr):\n","    arr = jpeg_def(arr, quality=40)\n","    arr = gaussian_def(arr, k=3)\n","    arr = median_def(arr, k=3)\n","    arr = bit_def(arr, bits=5)\n","    return arr\n","\n","def to_tensor(arr):\n","    return torch.tensor(arr/255.0, dtype=torch.float32).permute(2,0,1)"],"metadata":{"id":"msBIRHvOSnUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#PGD Evaluation Function (With / Without Defense)\n","def eval_pgd(model, dataloader, eps, hybrid=False):\n","    correct, total = 0, 0\n","\n","    for x, y in tqdm(dataloader, leave=False):\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","\n","        adv = pgd_attack(model, x, y, eps=eps)\n","\n","        if hybrid:\n","            arr = (adv[0].permute(1,2,0).cpu().numpy() * 255).astype(np.uint8)\n","            arr_def = hybrid_defense(arr)\n","            tens = to_tensor(arr_def).unsqueeze(0).to(DEVICE)\n","\n","            with torch.no_grad():\n","                out = model(tens).mean(dim=(2,3))\n","        else:\n","            with torch.no_grad():\n","                out = model(adv).mean(dim=(2,3))\n","\n","        pred = out.argmax(dim=1)\n","        correct += (pred == y).sum().item()\n","        total += 1\n","\n","    return 100 * correct / total"],"metadata":{"id":"eG3b8phMSsiF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run PGD Evaluation\n","EPS_LIST = [0.0, 0.02, 0.04, 0.06, 0.08]\n","\n","pgd_no_def  = []\n","pgd_hybrid  = []\n","\n","print(\"ðŸ”¥ Running PGD evaluation...\\n\")\n","\n","for eps in EPS_LIST:\n","    print(f\"Evaluating eps = {eps}\")\n","    pgd_no_def.append(eval_pgd(base_model, val_dl, eps, hybrid=False))\n","    pgd_hybrid.append(eval_pgd(base_model, val_dl, eps, hybrid=True))"],"metadata":{"id":"Mj0VFxA3SzjV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#PGD Results Table\n","print(\"\\nPGD Robustness Comparison\")\n","print(\"Epsilon | No Defense (%) | Hybrid Defense (%)\")\n","print(\"---------------------------------------------\")\n","\n","for i, eps in enumerate(EPS_LIST):\n","    print(f\"{eps:6.2f} | {pgd_no_def[i]:14.2f} | {pgd_hybrid[i]:17.2f}\")"],"metadata":{"id":"6zd0SrFAS4mk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#PGD Accuracy Plot\n","plt.figure(figsize=(7,5))\n","plt.plot(EPS_LIST, pgd_no_def, marker='o', label=\"PGD (No Defense)\")\n","plt.plot(EPS_LIST, pgd_hybrid, marker='o', label=\"PGD (Hybrid Defense)\")\n","plt.xlabel(\"Epsilon\")\n","plt.ylabel(\"Accuracy (%)\")\n","plt.title(\"PGD Attack Robustness with Hybrid Defense\")\n","plt.grid(True)\n","plt.legend()\n","plt.show()"],"metadata":{"id":"9UtheUABS9JV"},"execution_count":null,"outputs":[]}]}